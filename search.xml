<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基本数学公式推导]]></title>
    <url>%2F2019%2F11%2F17%2F%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[log sum的不等式(下界) 如果在损失函数中存在离散的变量, 例如离散的变量作为隐藏变量, 那么BP算法无法应用, 通过构建其下边界来替换之前的的损失函数. log sum下边界参考网址]]></content>
  </entry>
  <entry>
    <title><![CDATA[调参技巧]]></title>
    <url>%2F2019%2F11%2F12%2F%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[调参技巧 grad clip data nomalize的方式: mean std 或者scale到(0,1) batch_size loss用l2还是开方l2 dropout的概率 参考网址知乎关于调参的建议]]></content>
  </entry>
  <entry>
    <title><![CDATA[神经网络压缩]]></title>
    <url>%2F2019%2F11%2F09%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[前言剪枝NAS 量化int8, int4 稀疏效果不佳?? 蒸馏KD, fitnet,L2回归特征]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题总结]]></title>
    <url>%2F2019%2F09%2F18%2F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[查找数据题目描述二维数据中元素行和列都是递增排序,对于给定的二维数组A和确定元素，判断该元素是否在数组内部. 思路一开始想到的是暴力的解法，但是空间复杂度过大. 之后考虑数组特有的性质, 发现数组中点a的左上方比a都小，右下方都比a大, 一开始想着用递归的思想(从左上方向右下方遍历)，但是会存在重复计算点的情况。最后参考网上的教程，发现从右上方向左下方遍历，由此带来的好处是当点a确定时, 输入的值小于点a, 那么可删除点a的一列。本质上也是使用递归的思想的，只是修改之后的每次进入下一轮还是一个完整的矩阵，并且没有重复的点。 编程向左向右不断移动, 删除列时，左边的节点重新成为新矩阵的右上角，因此向左移动一个点，反之，向右移动一个点. 反思 考虑数组本身有的特性 别死脑筋，考虑数组只能从左上方向右下方遍历, 还能考虑从右上方向左下方遍历的.]]></content>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2019%2F09%2F16%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[numberical feature missing value binary features: 1 for positives, 0 for negatives numeric features: encode as a big values encode by splitting 2 columns: one for binary feature, another one is replaced by mean or median categorical feature missing value encode as an unique category: NAN is informative use the most frequent category level: do not contians any information for target turn categorical features into numeric features labeled encoding: scikit-learn: LabelEncoder range: tree-based methods One hot encoding: scikit-learn: DictVectorizer,OneHotEncoder range: K-means,Linear,NNs frequency encoding: encoding categorical according their relative frequency]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础]]></title>
    <url>%2F2019%2F08%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[平时的积累，掌握一些机器学习基础的知识。 统计机器学习基础 输入空间，输出空间，特征空间 联合概率分布: 基本假设(独立同分布产生) 假设空间(hypothesis space): 所有可能的模型的集合 决策函数:y=f(x) SVM(支持向量机) 是什么? 二分类模型 特征空间上间隔最大的线性分类器() AdaGrad motivation different feature has its own learning rate many features are irrelevant rare feature often very informative idea: frequent feature use low learning rate infrequent feature use high learning rate]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch基本操作]]></title>
    <url>%2F2019%2F08%2F19%2Fpytorch%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[记录pytorch的常规的操作,例如矩阵乘法 核心将公式转化为矩阵的操作,用来探索如果用矩阵之间的操作来实现. 常规操作公式:$$a=tanh(rWx+Uy+b)y\in R^{batchnum_stephidden_number}x\in R^{batchnum_stephidden_number}r\in R^{batchnum_stephidden_number}a\in R^{batchnum_stepnum_step}$$ 做法: rWx采用1234rW = linear(hidden_number,hidden_number)(r) #[batch_size,num_step,hidden_dim]rWx = pytorch.matmul(rw,y.transpose(1,2)) #[batch_size,num_step,num_step]Uy = pytorch.expand(-1,-1,num_step)(linear(hidden_dim,1)(y))在Sequence-to-Nuggests中由于将Uy视为每个字自身单独作为边界词, 因此需要将transpose(1,2)(Uy)之后再相加,如果直接相加,维度2上的值都相同，无法体现每个词单独作为边界词。 gather():能根据index从高维数据中选取对应的数据 当有什么复杂的操作的时候，首先思考是否可以添加一个维度来解决expand,transpose 将[L,h,K]转化为[L,h,h]时候，如果维度1之间需要共享维度2的内容,那么linear(K,1)+expand+transpose,适用范围在于维度1和维度2代表的物理意义是相互独立的,换言之无论维度1上的值怎么变化,维度2上的值不变的,一般出现tanh(rWx+Uy)适用于这种情况. 如果维度1不需要共享维度2的内容,直接linear(K,h) tensor的复制 clone(): 深度复制,修改当前tensor的值不会影响到被复制的值, detach(): 浅复制,复制的值是共享内存，但是复制的tensor的require_grad为false,换言之，复制的tensor从计算图中分离.]]></content>
  </entry>
  <entry>
    <title><![CDATA[NER相关的概念]]></title>
    <url>%2F2019%2F08%2F10%2FNER%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[记录实体抽取的一些常见概念 中文实体抽取中文实体抽取和英文的不同之处: lack strong indications of entity names no delimiter chinese entity name are highlu context 相同词，但是是非实体和实体 相同次，但是是不同类型的实体 NLP基本任务分类https://blog.csdn.net/lz_peter/article/details/81588430 实体抽取相似的任务 named entity recognition keyphrase boundary classfication indentification of multi-word expressions(MWE) neural sequence label POS(part of speech) Concept detection, Attribute name extraction, and Phrase mining(CAP) 多任务学习(multi-task) 参考网址: 多任务学习 机器学习角度视为规约迁移(inductive transfer),其是通过规约偏移(inductive bias)来改进模型,使其更加偏向于某种假设(使模型偏向于能同时解释多个任务的解)，从而增强模型的泛化能力. 经典的inductive bias有L1正则化，使模型更加偏向于简单的解，例如相比于三次解，更加偏向于二次解。 视为远距离监督信号(a distant supervision signal)的半监督学习. 单纯在少量目标数据上训练会导致过拟合问题，引入辅助数据集能提高模型的泛化能力. 引入辅助输入数据的边缘概率分布(the margin distribution of auxiliary input data) 多任务学习能视为正则化(reductions in Rademacher complexity in multi-task architectures over single-task architectures) 实体抽取的方向 通过联合实体之间的关系来提升实体抽取的效果，显性加入关系label(weekly distant supervision),隐性seq2seq模型来提取实体(例如pointer network) 更好的探索label dependency, 之前的方法基本在探索tokens之间的依赖关系，label dependency基本上只采用crf模型. entity boundary, 之前做实验的效果发现显性确定实体boundary能提升实体抽取的效果.]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文写作逻辑]]></title>
    <url>%2F2019%2F08%2F07%2F%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[记录一下论文中每个章节的写作逻辑和套路. 通用部分 总(笼统) –&gt; 细化, 例如先提出一个大问题(效果不好)，之后细化到具体问题(哪里的问题). 先提出总的方法(方法的基本思想)，之后细化方法(具体到网络结构模型) 提出xxxx模型 to xxx ,then xxx, 为什么要这么做(原因/目的), 这么做和其余方法比较有什么优势. 语句逻辑: 是什么，提出什么，目的是什么，挑战是什么，意义是什么，重要性是什么，作用是什么，怎么做，为什么这么做， 有什么效果，为什么有效果，应用是什么。 段落逻辑: 做了什么内容 内容是什么 内容的优势，为什么会有优势 作用是什么(to) 应用是什么，能够用来干什么 是通过xxx方式来获取的（具体的网络结构）Introduction 介绍任务和意义(是什么，目的是什么，应用是什么) 介绍任务已有的方法 已有方法面临的挑战: 然而现实情况是xxx，基于xxxx假设是weekly, 我们相信xxx不能xxx和xxx 阐述一个笼统的问题,例如导致了性能效果不好，不一致问题等 细化问题本身，并说明原因 应用它或导致xxx问题 实例化，特别是xxx任务，因为xxx原因 针对挑战，提出创新思路, 阐述基本思想和网络结构,每个步骤的基本思想(例如换模型的时候，先阐述待替换模型的优势，但是很少利用在xx领域，) 实验结果验证方法的有效性 贡献点: 网络模型，新颖的方法，实验结果, 模型的优点(例如通用性广,能应用到其余任务) 方法 先介绍前人的方法，之后受到他们的启发之后提出自己的方法, 其中提出的方法的步骤可以是: 先通过一些简单语句将核心概念提出来，之后再强调概念的重要性，然后详细阐述概念的具体内容(如果有什么假设，写在具体阐述之前) 通过类比的方法来引出自己的方法，例如,xxx是怎么做的，similary, 我们通过xxx做xxx inspired by the success of xxx, 我们采用xxx去干吗 先思想，后步骤(First ,then, ) 先提出xxx内容，其基本思想是啥 对引入的内容详细说明，例如重要性，假设，意义，挑战等等 再次细化，内容有那几部分构成，如何将这几部分进行融合 实验部分实验数据阐述数据本身的统计值，例如句子的数量，标签类型个数, domain(领域), 特征(数量级,多样性), 来源,训练集，验证集和测试集的划分 实验设置 word embedding: 大小，来源(例如 Glove), words out of the vocabulary(例如:未登录词的初始化), 微调(例如根据频率大小删除一些词) 初始化的方法(例如Xavier等), batch-size, epoch, learning rate strategy, dropout 等 网络结构描述 实验结果 main result/performance comparision: 阐述章节的目的，概括性描述 阐述对比方法的大类别(如果有的话), 例如LSTM-based, CNN-based等, 总之阐述引入了那些方法，以及为什么要引入上述方法 结果呈现在tabel x, 阐述tabel中的表的属性(例如F1 score, precision, recall), 阐述表中那些方法是自己实现的, 总之就是要将表中的属性能描述的都描述了 阐述实验结果(例如xxx achieve xx, outperform xxx), 之后阐述实验结果好的原因 Factor Analysis/Ablation Study: 写这章节的目的, 例如我们已经观察到加入一些方法能让模型的性能变好，为了能够更好的研究xxx,我们构造一些实验来进行分析, 先阐述目的，之后为了达到目的，我们构建了某些模型(例如介绍某个模型删除一些内容)进行对比。 先描述实验结果，之后在indicate结论 error analysis: 逻辑方法 目标(基本思想)是什么，recent advances of xxx, 近期的xxx方法能够xxx. (思想到做法) 直观上的思想是什么，Inspired by xxx, 具体的做法是什么. 大到小的思想, 宏观的概念，之后详细解释概念的来源/定义。 先阐述具体的步骤，之后总结步骤的思想. 假设(assume)xxx, 为了(to xxx), 我们应该xxxx. 先阐述基本思想，之后阐述作用/好处/意义最后阐述方法. 先阐述对象本身的特点(例如基本思想，好处，方法等)，之后将对象嵌入到更大的对象(例如步骤)中进行阐述. 首先引出要就研究的对象(例如从通用方法引出)，之后阐述对象的本身的特点(例如要求)，接着阐述方法 在文章开始(introduce)的时候要简单阐述(一般不涉及具体的方法,by xxx尽量不要出现), 之后method章节的一开始要尽可能详细阐述方法(by xxx, with xxx over xxx,to xxxx,from xxx)，但是不涉及具体的公式，最后形式化阐述时候，尽可能用公式来阐述。 首先阐述最常用的方法或者前人的方法，之后阐述其缺点，如果没有缺点的话，可以用In pratice来进行过度到自己的方法，最后再公式化阐述自己的方法. 对于某些特殊的超参数，可以通过当该参数接近于0是什么情况，当该参数接近于无穷时是什么情况，来增加篇幅 存在方法的做法/基本思想 + 存在的问题 + 提出解决措施. 目标是什么 + 现在的方法不能到达目标/存在一些问题/为什么要提出我们的方法 + 引出我们的方法 + 最后阐述这么做的好处/作用或者原因(一般用于扩充文章篇幅). 一般是先描述主流程步骤，如果要强调步骤中的一些关键点，一般是在最后用For xxx, we do xxxx. 不同章节的方法的主语需要不断变换 引入前人的方法的时候，Inspired of xxxxx 一开始阐述方法的时候，可以将对象作为主语来阐述，也将将对象作为模型的一部分来阐述(局部和整体) 说明理由的时候，还可以反向说明理由，如果采用和我们相反的方法会导致什么问题。。。 首先说明要加入什么内容，之后简单阐述加入内容的特点(例如怎么训练，有哪些优点)，最后阐述加入的方式(例如concatenate,add等)]]></content>
  </entry>
  <entry>
    <title><![CDATA[最短距离算法汇总]]></title>
    <url>%2F2019%2F08%2F03%2F%E6%9C%80%E7%9F%AD%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[详细介绍Dijkstra, floyd, bellman-ford 和SFPA算法. Dijkstra算法Dijkstra算法是处理单源最短最短路径的算法, 通过贪心策略来选择目标节点到每个节点的最小距离。首先选择当前状态(未访问节点集合)中与起始节点最小距离的节点，并计算它与其余节点的距离, 若其距离小于原始距离数组对应节点的值，则更新距离数组, 反之，不更新，不断重复上述过程直到访问完所有的节点.由于其采用贪心算法的思想, 具有贪心选择性和最优子结构性质，若图中存在负边会违反其贪心选择的性质，换言之当前最优不是全局最优。例如:12345A --&gt; B : 1A --&gt; C : 3C --&gt; B : -3A到B的正确最短路径是3 + (-3) = 0, 而根据Dijstra算法得到的结果为1 Dijstra算法的时间复杂度为$Time = O(V)T_{extract_min}+O(E)T_{decrease_key}$, 其中$V$为图中节点个数，$E$为图中边的个数.由于教程中涉及的版本是用数组来实现Dijstra算法，导致一直无法理解上述时间复杂度的意思. 直到本人在wikipedia上找到其关于优先队列代码实现才能理解，其代码如下所示:12345678while Q is not empty: // The main loop u ← Q.extract_min() // Remove and return best vertex for each neighbor v of u: // only v that are still in Q alt ← dist[u] + length(u, v) if alt &lt; dist[v] dist[v] ← alt prev[v] ← u Q.decrease_priority(v, alt) 抽象的优先队列通过数组、二叉堆(binary heap)和斐波那契堆(Fibonacci heap)数据结构来实现，他们各自的时间复杂度如下所示: Floyd算法Bellman-ford算法SFPA算法]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文写作中的好词好句]]></title>
    <url>%2F2019%2F08%2F01%2F%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E4%B8%AD%E7%9A%84%E5%A5%BD%E8%AF%8D%E5%A5%BD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[记录论文写作中的好词好句 连接词at the expense of: 以…为代价Nevertheless: 尽管如此let along: 更不用说in the extreme: 极端情况下on a slightly different approach: 在一个稍微不同的方法To this end: 为此Intuitively: 直观而言In addition: 除此之外As a result: 因此/由此namely: 也就是Meanwhile: 同时In this way: 用这种方式Under this mechanism: 在这种策略下generally: 一般情况下(详细介绍的连接词)In practice: 在实践中according to: 根据that is: 那就是(一般加的是具体的步骤)concretely: 具体的话formally: 形式化Alternatively: 替换地greedy: 贪婪的Accordingly: 因此in addition: 除此之外especially: 特别In comparison to: 和xxx比较by contrast: 相反particularly: 尤其，特别more or less: 或多或少corresponding to: 相应的,对应的basically: 大体而言,基本上in the next phase: 在下一个阶段in more detail: 更加详细而言among: 在xxx之间as well as: 还,以及for brevity: 为了简介more precisely: 更加具体而言with respect to: 关于rather than: 而不是throughout: 贯串be coupled with: 结合,与…联合rather surprisingly: 出人意料 常用动词rule out:排除entail: 包含readjustment:调整interfere: 干扰resistance: 抵抗mitigate: 减轻dissect: 解剖inject: 注射,添加harness:利用pursue: 追求refer to: 引用wrap with: 用xxx包裹overlook: 忽略fullfill: 完成diminish: 减少pack: 包装dismiss: 失去bypass: 绕过interfere: 干涉parametrize: 参数化comprise of: 包含be paired with: 和xxx匹配be benchmarked on: 以xxx为基准yield: 产生subsume: 归入portrait: 描绘guide: 指导adjoin: 紧挨denote xx as: (公式)表示为be represented as: 表示为 (公式)enable xx to xx: 能够干吗focus on: 关注yield: 实现degrade: 降低investigate: 研究 Inspired by: 灵感来自…draw back: 拉回/撤回/后退bridge the gap: 消除隔阂converge: 收敛employ: 采用optimize: 优化denote it as: 将it表示为xxx(一般后面加的是符号)apply: 采用define xx as: 定义什么为xxx(一般后面加公式)assure: 保证confront: 解决trap in: 陷入expose: 揭露monotone decrease: 单调下降Compared with them:equipped with: 装备/用xxx标记mitigate: 减轻/缓和indicates: 暗示/揭示conjecture: 猜测veiw xxx as: 将xx视为induce: 引入treat the rest xx as: 将剩余的xx视为xxxcarry out: 开展report: 汇报demonstrate: 展示/说明wipe out: 清除manifest: 表明incorporate: 吸收/引入prevent: 阻止creep into: 潜入collaborate with: 和xxx合作play a critical role in xx: 在xxx起重要角色flow from xx: 从xxx流出,xxx一般是数据基,例如source data, target data等encode xx to xx: 把xxxx编码成xxxdecode xx by: 通过xxx解码成xxbe employed to: 被采用去干吗be introduced to: 被采纳去xxintegrate: 整合eliminate: 消除absorbe: 吸收suppress: 抑制be compatible with: 和xxx能够共存的/兼容的differentiate: 区分ought to be relieved: 应该被解除dive deep into: 深入探索extend xx by size xxx: 扩大了xxextend xx to size xxx: 扩大到xxstruggle with problems: 和问题作斗争 常用形容词concurrent: 并存的，同时发生的multifarious: 多种多样的conceivable: 可信的detimental: 有害的indispensable: 不可或缺的tractable: 易处理的context-agnostic: 上下文无关的deformable: 可行变的vanilla: 普通的analogous: 相似的intractable: 困难的diversely: 不同的feasible: 可行的distant: 遥远的infeasible: 不可行的salient: 突出的subsequent: 后续的extremely: 极端off-the-shelf: 现成的intensively: 强烈地compositional: 组成的 straightforward: 直接的dominant: 主要的/主导的separate: 不同的/单独的rich: 丰富的over: 在xxx之上superior: 更好的vanilla: 普通的navie: 天真的/普通的be crucial for: 至关重要的available: 可利用的(和exsiting意思相近)be related to: 和xx 相关insufficient: 不充分be adequate to: 充分的successively: 迭代地dominant: 主导的/显著的fine-grained: 细粒度的versatile: 多样的/广泛的appropriate: 合适的subjective: 主观的authentic: 真正的synthetic: 合成的ambiguous: 模棱两可的scarce: 缺乏的 常用名词patent: 专利varying degrees: 不同程度pros and cons: 利弊salience: 显著性novelty: 新颖性false statement: 错误的陈述from scratch: 从头开始,白手起家coherence: 一致性heuristic strategy: 启发式规则quadratic: 二次heuristics: 启发式inferior performance: 较差的性能interleaves: 交织trend: 趋势domain-agnostic: 领域无关的rationale: 理论基础adaptability: 适应性proposals: 实体提议(建议)natural and intuitive way: 自然和直觉的方式a skewed distribution: 不均匀分布morphological patterns:形态特征(词法)morphology: 词法POS: 词性semantics: 语义syntax(grammatical): 语法intrinsic skewness: 偏移程度phonetic and phonological features: 语音特征orthographic feature: 拼写特征horizon: 范围logogram:标示符号alphabet:字母表language nature: 语言性质downstream application: 下游应用flexibly: 灵活sub constitute: 子成分interaction: 相互作用/相互影响discrepancy: 不同/差异train criterion: 训练标准The main framework: 主要的观点/思想.condition: 条件situation: 环境strategy: 策略categorical distribution: 类别分布temperature: 温度(一般用于缩放中，例如整流网络中)the goal of: xxx的目标symbol: 信号circumstance: 环境the first term : 第一部分abbreviation: 缩写a linear layer: 线性层a log-softmax layer: 对数softmax层log-probabilities: 对数概率category: 类别regular order: 常规顺序named entity tag set: 命名实体集合focus word: 焦点词multi-class classification: 多类别分类器with the length of: 长度association: 关联label dependency: 标签依赖standard deviation: 标准差the impact of xx: xxx的影响in the firt block: 在第一个模块fluency and adequacy: 流利性和充分性complementary: 补充logographic language: 语标式语言(表意文字)temporal interest: 暂时的兴趣consistent preferences: 长期的爱好recipe: 方法disjoint feature: 不相交的特征Filtration: 过滤superiority: 优势disjoint: 不相交information gain: 信息增益 常见副词exclusively: 完全地utterly: 完全地elegantly: 优美地，高雅地significantly: 明显地merely: 仅仅purely: 纯粹地relatively: 相对地recently: 近期beyond: 超过such xxx: 这种xxxgradually: 逐渐further: 进一步in reverse: 相反separately: 单独地significantly: 极大地dramatically: 明显地 常见语句provide a solution: 提供解决措施achieved highly competitive accuracies: 实现了高度的竞争精度.bring benefits to: 给xxx带来益处node communication pattern: 节点交流模式recent advances of xxx: xxx的最新进展be informed about: 了解under the condition it will face: 在面临xxx条件下As the model converges gradually: 随着模型逐渐收敛one way xxx, another xxx: 一个方法xx, 另一个方法xxx.the following steps are involved in our approach: 我们方法采用下列步骤perform the following prediction of: 执行接下来的预测Note that xx: 注意xxxIn the process of: 在xxx过程中Borrowing ideas from but being different from: 从那边获取idea, 和inspired by xxx相同作用The objective is to maximize the probability of xxxx based on maximum likelihood estimation: 介绍损失函数的时候achieves competitive performance with: 实现有竞争的性能We conjecture that the superiority may come from: 说明性能提升的原因is consistent with our claim: 和xxx是一致的is a representation of xx: 是xx的表示take into account: 考虑到the relationship between adjacent tags: 临近标签之间的关系Following the work of: 根据xxx的工作investigate the influence of xxx: 为了研究xxx带来的影响,…the performance is improved by xxx:further archieve xxxx improvement:this indicates, it show that, we conjecture that,update the learning rate as xx: 以xxx的训练速率更新训练select best version with the highest performance on xxx and report the corresponding performance on xx: 选择最好的版本to reduce model bias: 减少模型偏差mini-batch stochastic gradient descent with momentum: 随机梯度下降方法existing state-of-the-art approaches: 存在的当前最优的方法exploit external knowledge to boost performance: 利用额外的知识去增强性能it should be noticed that: 需要注意的是obtain comparable or even slightly superior performance: 获取可以比较，甚至是超越的结果defeat it at a great/substantial margin in terms of: 就xxx而言, 以很大的优势击败它obtain the state-of-the-art NER performance on xx: 在xxx上取得了最有效果the absolute performance drops: 绝对的性能下降different scales of local context information: 不同规模的局部上下文信息comparison of xxx with xxx: xxx和xxx的比较xxx improve F1 score from xx to xx as compared xxx on xx: 和xxx相比，F1值从xxx提高到了xxxboosting performance as compared with xxx, showing xxxx improvement on xx: 和xxx比较性能增强了xxx is very effective fo xx task: 对于xx任务, xxx是非常有效的it proves that: 证明了什么improve the performance on xx with the help of xx : 在xxx的帮助下提高了性能 Abstract review and discuss the pros and cons of each category: 审视并讨论每个方法的利弊.introduction can capture global context information and local compositions to xxx through recursively aggregating mechanism: 通过递归整合机制来获取全局上下文信息和局部整合信息来xxxx which is problematic without seeing the remaining characters: 这个是困难的在没有看到剩余的字符. can constitute words with the characters to both their left and their right: 同时能够和它左边和右边的字构成单词. the recognition of named entities with overlapping ambiguous strings is even more challenging: 识别重叠的歧义字符串的实体是更加有挑战力. xxx using xxx has attracted research attention: xxx使用xxx已经收到了研究者的关注。 xxxx and xxx negatively impact the identification of named entities: xxx和xxxx会负面影响命名实体的识别。 their approach will arguably suffer: 他们的方法可能会受到影响 to tackle the aforementioned drawbacks: 解决上述问题 decent performance: 得体的性能 xx can benefit various subsequent xx: 为后续各种xxx带来好处 a number of approach have been proposed to xxx: 大量方法已经被提出去xxx xxx have been studied for many years and various methods have been proposed: xx已经经过很多年的研究，大量方法已经被提出 many effort have been dedicated to the field: 很多人致力于这个领域 hard to adapt to other xxx: 很难调整到其余领域 augment xxxx with xxx: 用xxx来增强 utilize xx when xx: 当xx时候利用xxx adopt: 采用 heavily rely on hand-crafted feature or task-specific resources: 高度依赖手动特征和任务指定资源 with the development of xxx: 随着xxx的发展 along with its variant: 和他的变体 bring great success in xxx: 带来了巨大的成功 xx be proven to be powerful in xxx: 证明有能力xxx basic components of xxx: 基础组件 the vast majority of xxx: 绝大多数的xxx especially: 特别是 rnn–&gt;sequential manner: 序列方式 the subsequent steps depend on previous ones: 后续步骤依赖之前的词 compute xxx incrementally over each word in a sentence: 对句子中的每个词递增进行计算 the computation at the current time step is highly dependent on those at previous time steps: 当前步骤的计算高度依赖之前的步骤 memorise words from far away in the sequence: 记忆远端的词 speed up: 加速 inherently sequential nature: 固有的序列性 preclude: 阻止 hinder: 阻碍 permit: 允许 parallelism: 并行 fashion: 方式 cnn -&gt; local context information: 局部上下文特征 rnn -&gt; long-term context information: 长期上下文特征 the receptive field of xx: 接受域,对于CNN而言 variant-sized text sentences: 变体长度的句子 regard to: 考虑 versus: 和xxx相比 at one time: 一次/曾经 be customized into: 被定制为 transform the word features from embedding space to the latent space: 词特征从embedding空间转化为潜在的空间 be built on xx: 建立在xxx之上 end-to-end manner: 端到端的方式 can compete with: 和xx相比有竞争力 our contributions are summarized as follows. 我们的贡献如下所示 with a more powerful capacity of doing xxx: 有更加强大的能力去干嘛xxx leverage: 利用 by exploiting the association between xx and xxx: 利用xxx和xx的联系 outperform other popular methods by a large margin in term of xxx: 就xxx而言, 大幅度超越其余方法 this is first attempt to xxx as xx to xxx: 第一次尝试将xxx作为xx去干嘛 is popularly used as benchmark in related work: 在相关工作作为基线 excellent performance: 卓越的性能 correlation: 相关性 surpass xxxx a measured by xxx score: 按照xxx测量，xxx超越了xxx motivated by those observations: 基于这些观察 recent research efforts have shown that xxx: 最近的研究表明xxx enhance the ability of xxx in xx doing xx : 在xx加强xxx做xx的能力 as a solution to those issues: 针对这些问题的一个解决 address the issue of feature sparsity: 解决了特征稀疏问题(与传统的机器学习方法相比，深度学习的优势) non-local features that capture semantic patterns that cannot be expressed using xxxx: 能获取的到语义信息的非本地特征，这些特征不能被xxx表示 manual feature templates: 人工构造的模板 cover the most useful indicator pattern: 覆盖最有用的指示器模式 it is challenging to be integrated into end-to-end learning fashion: 非常有挑战整合到端到端的模型中 the estimation of xxx highly relies on xxx: xxx的分析严重依赖与xxx made extraordinary impacts on the solutions of a broad range of problems: 在解决一系列问题上有着非凡的影响 引入参考文献的方式: which is shown to be effective in xxx task. which is better than previous works. Method hinge-rank loss: urge the probability for the right label higher than probability for the wrong labels: 促使正确label的概率高于错误label的概率 novel entities constantly and rapidly emerge: 新颖的实体不断(constantly)迅速(rapidly)出现 treat xx corresponding to xx as xxx: 将和xx对应的xx视为xxx gated: dynamically select and compose features: 动态选择和组成特征 assign higher weights to various type of feature: xxx-augmented gating mechanism: xxxx增强的门机制 element-wise dot-product: 元素级别的点乘 dynamically control the ratios at which source and target contexts contribute to the generation of target words: 动态贡献给目标词产生的源上下文和目标上下文的比率 dynamically select the amount of context information in the decoding process: 在解码过程中, 动态选择上下文信息量 the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts: 门测量源和目标的上下文，输出0到1的比率去决定利用的两个上下文的信息比例. looks at input signals from xxx: 从xxx查看输入信号 the gate assigns higher weights to xxxx and lower weights to xxx: 门机制分配更高的权重给xxx, 更低的权重给xxx control the amount of memory content utilized: 控制利用的内存内容量 looks at the xxx and decides to depend more heavily on xxx: 观察xxx, 然后分配更高的权重给xxx to abtain an optimal mixture(gate) lstm: long-distance context feature: 长距离上下文特征 CRF: define a conditional probality distribution over label sequences given a particular observation sequences: 在给定观察序列，定义在标签序列上的条件概率 neighboring labels usually have string dependency: 相邻的标签之间存在依赖关系 jointly decode the label of characters in a sequence rather than decode them independently: 联合解码序列中字符的标签，而不是独立解码 structed prediction model: 结构化预测模型 improve the prediction ability of the model by taking the neighboring prediction into account: 通过将周边的预测结果考虑来提高模型的预报性能。 transition scores and emission scores: 转移分数h和发射分数(状态分数) attention: dynamic decide how much information and which part of a token to use. 动态token的多少信息和哪部分信息被使用 cnn: local and shift-invariant features: 局部偏移不变特征 local dependency patterns among xxxxx: 本地依赖模 attend to the ones relevant to xxx: 关注那些和xxx相关的词语 word boundary aware contextual character representation: 词边界意识的上下文字表示 rich structure information: prefix, suffix, n_grams 丰富的结构信息，前缀，后缀，n-gram encourage domain invariant features: 加强领域不变特征 gradient reversal layer: - make the two feature distributions as indistinguishable as possible: 让两个特征分布尽可能无法区别 CRF xxx play a vitally important role in xx : 在xxx领域xxx扮演了重要的角色 the interactions make xx and xxx aware of each other and help to xxx more precisely: 交互让彼此意识到对方，从而能帮助xxx更加精确 conduct attention based interactions in encoder and subsequently decoding with two series of xxx: 基于交互来构造注意力，之后用两个系列来解码 as a solution to this issue: 作为解决这个问题的措施 the transitional constraints among sequential labels: 考虑序列标签之间的过渡约束 trade off how much information the network is considering from xxx with xxx: 网络考虑从xxx交易(利用)多少信息。 intermediate representation feature: 中间表示特征 different dependencies on xx for different words: 不同的词对xxx有着不同的依赖 better represent the information needed to solve a paticular problem: 更好的表示需要被解决的信息 provide sufficient clues to infer its meaning and determine whether it is a name: 提供足够的证据去推导它的意思和决定它是否是一个名字. 引入某个变量的句子: investigate whether the effect of dependency would be diminished by the contextualized word representations: 调查是上下文特征会减少句法分析的影响。 entity mention: an entity mention is a text span that refers to an entity object: 实体引用是值实体对象的文本范围. indicates that there is at least one sub-sequence of s matching a word of the lexicon and beginning with c1: 说明至少有个一个子序列s和字典中的一个词匹配，并且是以字符c1起始。Experiment the dashed delimited box: 虚线分割框 utilize xxx while avoiding side effects by xxx:在避免xxx的负面影响下,能够充分利用xxx capture textual semantics from different perspectives: 从不同方面来获取文本语义 compare xx with reported result from previous works: 和之前方法结果进行比较 remove one model at a time: 一次移除一个模型 The result can be interpreted that: 结果可以被解释为xx automatic learning of high-level features is advantages of deep learning methods: 深度学习的优势在于能自动学习到高维度特征 learn domain knowledge from the raw text and capture sufficient context information from a sentence: 从原文中学习到领域知识和从句子中获取到足够的上下文信息 detect the entity without sufficient context information: 没有足够的上下文来检测实体 annotated training set contains the tagging inconsistency: 标记的训练集包含标签不一致问题 incorrect boundary: 不正确的边界 missing xxx mention: 未识别出实体,例如缩写 not a xxx mention: 识别出的内容不是实体 it is difficult to xxx: xxx是非常困难的 attribute this to the fact that our approach resorts to xxx: 我们将这归因于我们方法采用的xxx a new perspective: 新视角 this observation coioncides with our previous claim: 这观察结果和我们之前的主张一致 take int account domain adaptation: 考虑领域适应性 corresponding to each xxx task, xxx achieves relative F1 improvement of xxx: 根据每个任务,我们分别提升了xxx性能 the performance of xxx drops dramatically in xxx: xxx的性能在非正式文本中急剧下降 data distribution gap: 数据分布差异 noisy nature of informal text: 非正式文本的嘈杂性质 emerging entities with novel sufaces: 具有新颖表面的新颖实体 reduce the distance in data distributions between: 在xx和xx之间减少数据分布的距离 less substantial: 不太重要 converge to a plateau range: 集中在较高的范围内 the usefulness of using source data to enhance target labeling： 源数据对于增强目标标记是有用的 difference between the two datasets can hurt naive transfer learning: 两个数据的不同会损简单的迁移学习 contains conflicting information from different text genres and task requirements: 不同文体和任务要求下，会存在冲突信息 domain contrast: 领域差异 our model gives significantly better results by gleaning such contrast: 我们模型通过收集差异来达到更好的结果 Table 3 also shows a comparison with: 表3显示了和xx的比较 giving the best results in the literature: 在文献中给定了最好的结果 intuitively play an increasingly important role for: 直观上会发挥越来越重要的作用 the gap between the two methods becomes smaller: 两个方法的差距越来越小 show the effectiveness of xx : 显示xxx的有效性 in accordance with the conclusion of : 和xxx的结论一致 shows the strength of: 显示了xxx的能力 require more computation resources: 要求更多的计算资源 obtain reasonable result with respect to xxx on xxx: 在xxx上获得有关xxx的合理结果 how entities are contextualized with and without entity type information: 在有和没有实体类型的情况下,实体是怎么样进行上下文化 learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to the positions in an input sequence: 学习输出序列的条件概率,其中的元素是与输出序列中的位置相对应的离散标记。 outputform xx model but achieves inferior preformance compared to the xx model: 超越了xxx模型,但是和xx模型相比实现了次优结果 Conclusion consistent performance gain: 一致的性能增益 utterance:言语 human-in-the-loop: 人在循环中(一般用于半监督学习中) the risk of learning the disturbing features can be suppressed: 学习到干扰特征的风险将会被抑制 xxx is more expressive than previously thought: xx是比以前认为的更加有表现力 remedy the problem: 补救问题 a form of inductive transfer: 感应传递的形式 Related Work]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何高效阅读一篇论文]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E9%98%85%E8%AF%BB%E4%B8%80%E7%AF%87%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[掌握阅读论文的技巧，通过不断实践来锻炼自己阅读论文和归纳总结的能力. 笼统 带着问题去阅读文章，通过看摘要来提出疑问，带着问题去阅读文章. 批次阅读文章，每次筛选自己需要的内容。 不同文章对于同一知识点的解释不同，擅长用比较的方式来选择最优的解释。 阅读文章之后要有扩展思维的能力，大胆的想。 细节作者和大部分刚刚从事科研的小白一样，对于论文只会总结(summary)，总结只能够了解作者是怎么做，但是对于为什么这么做，为什么效果好等深层意义的探索需要通过对文章的批判(critique)来实现. 对论文的批判分为分析(analyze)文本，评估(evaluate)文本和写批判(critique)文本. 分析文本 作者的主要观点(point)是什么 作者的目的(propose)是什么 作者的潜在受众是谁 作者使用那些论据(arguments)来支持其提出的观点 作者提供那些证据(evidence)来支持论据 作者潜在的假设(assumption)和偏见(biases)是什么 论点就是作者提出的观点 论据分为事实论据(有代表性的事例前人研究的一些论文、史实及统计数字实验统计出的一些数据,举例子论证)和道理论据(名人名言、俗语谚语、自然科学道理、公式定律等) 评估文本 论据具有逻辑性吗 文本的组织结构好，逻辑是否清晰，是不是易于阅读 作者的事实(facts)是否准确 作者是否定义了某些重要的术语(term) 是否有足够的证据去支持论据 某个论据是否支持作者的观点 文本是否对于受众是合适的 文章是否存在或者反驳了完全相反的观点 文章是否有助于理解研究的主题(subject) 那些语句或者词能够引起你的反应，你的反应是什么, 记录下影响深刻的语句. 对于作者研究的主题，你最初的印象是什么，和本篇文章相似的文章，作者和讨论有哪些，他们和该文章的比较和对比. 对于本篇文章，你的思考是什么，你能提出哪些问题或者意见。 写评语 阐述作者的观点，并且指明观点在本篇文章中对应的段落 提出自己的观点，并且解释支持观点的论据，其中包括反对和支持的观点 对于自己提出的观点，找到对应文章中的段落 解释为什么找打的段落能够支持你的论点 举例参考如何有针对地高效地阅读一篇学术论文]]></content>
  </entry>
  <entry>
    <title><![CDATA[领域关键词]]></title>
    <url>%2F2019%2F07%2F31%2F%E9%A2%86%E5%9F%9F%E5%85%B3%E9%94%AE%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[本作者有一个很大缺点在于不太关注领域的关键词，导致搜索资料时由于缺少关键词而无从下手. 定义Euclidean Structure(欧几里得结构): 图像类似的矩阵排列结构.]]></content>
  </entry>
  <entry>
    <title><![CDATA[NER相关Paper]]></title>
    <url>%2F2019%2F07%2F28%2FNER%E7%9B%B8%E5%85%B3Paper%2F</url>
    <content type="text"><![CDATA[实体抽取相关的论文，此文章是对文章进行简单介绍，详细介绍点击对应链接。 NER Neural Models For Sequence Chunking: 首次将Pointer Network应用到Sequence Label来识别实体的边界, 其基本思想是利用pointer network迭代识别Chunk的endding position. 点击查看详情-]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2019%2F07%2F25%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[磁盘 df -hl: 查看磁盘空间占用清空,使用比例 du: 查看文件大小 max-depth: 显示文件的深度,例如0表示只显示当前文件内所有文件的代销，不显示其子目录的大小。 文件sh脚本声明 #!/bin/bash: 指定执行脚本bash 参数 $1和$2:分别代表输入命令行中的第一个参数和第二个参数. $#表示参数个数，$@表示参数的列表,以列表形式返回,不包括$0 IF语句 if 语句1234567if [ ];then # [ xxx ]中[]和xxx内容隔开必须要有两个空格. content1elif[ ];then content2else content3fi 算法比较:123456-gt: 大于-lt: 小于-ge: 大于或者等于-le: 小于或者等于-eq: 等于-ne: 不等于 文件系统相关测试:12345678[ -f $var ]: 是否包含正常的路径名[ -x $var ]: 文件是否可执行[ -d $var ]: 是否为目录[ -e $var ]: 文件是否存在[ -c $var ]: 是否包含字符设备文件的路径[ -b $var ]: 是否包含块设备文件的路径[ -w $var ]: 是否可写[ -r $var ]: 是否可读 字符串比较:字符串比较的时候，尽可能采用双中括号，单个中括号会出现问题.12345[[ $str1 == $str ]] 或者 [[ $str1 = $str2 ]]: 判断是否相等[[ $str1 != $str ]]: 判断不相等[[ $str1 &gt; $str2 ]]: 判断首字母的顺序[[ -z $str1 ]]: 如果str1包含空字符串，则返回真[[ -n $str1 ]]: 如果str1包含非空字符串, 则返回假 定义变量不能在中间不能出现空格,例如a=&#39;content&#39;中a和=之间不能出现空格. 循环语句1234567891011121314for var in &#123;1.100..2&#125;do xxxxdonefor file in $( ls )do xxxxdonefor((i=0;i&lt;10;i++))&#123; commands&#125; 123456789while(( i &lt;= 100))do xxxdonewhile [[ &quot;$num&quot; != 4 ]]do xxxdone 1234until [ conditions ];do xxxxxdone 变量 var=variable是赋值操作,var = variable是相等操作 length=${ #var}字符串长度 $SHELL是当前shell的名称 $UID是存在身份的变量名,例如普通用户和超级用户 let、(())、[]用来执行算术操作, 例如let a++,result=$[ a + b ],result=$(( a + b )),特别注意空格 高级的数学计算: expr,bc 文件描述符是与文件输入、输出相关联的整数,用来追踪已打开的文件. 1230: stdin (标准输入)1: stdout(标准输出)2: stderr(标准错误) 例如 ls 2&gt; file2将ls的输出的错误写入到file2文件中,ls 2&gt;&amp;1 output.txt表示ls的标准输出和标准错误都输入到output.txt文件中 运行 &gt;,&gt;&gt;分别表示重定向写入文件的覆盖和追加两种方式,&gt;等价于1&gt;, &lt;从文件中读取数据. $?表示命令执行的状态,$0表示命令执行成功,非0表示命令运行错误 /dev/null表示黑洞,输入到其中的内容都会被丢弃,/dev/stderr表示错误,/dev/stdout表示标准输出,/dev/stdin表示标准输入 tee表示既能写入文件又能通过管道(|)传入后续的命令中,例如cat a | tee -a out.txt | cat -n表示能将内容输入到out.txt,也能传递到cat -n中，注意的是tee只能接受stdout的内容 脚本内部文本块输入到文件中:1234cat&lt;&lt;EOF&gt;log.txtxxxxxxxxEOF 将cat&lt;&lt;EOF&gt;log.txt和EOF中间的内容输入到log.txt中 数组:${array_var[*]}读取全部数组的值,${!array_var[@]}读取数组全部的index,${ #array_var[@]}读取数组的个数，注意的是$后面的是大括号，不是小括号. 多个命令联合执行:cmd_output=$(ls | cat ...)采用管道的方式将多个命令整合,也可以使用/commands/1 调试 123set -xxxxxset +x set设置的中间部分为调试内容,只运行 read:用于和用户进行交互 command1 &amp;&amp; command2表示串行执行命令，当command1执行成功之后，才会执行command2 command1 || command2中||表示逻辑或，只有当command1不执行时，才会执行command2，常用来简化if else语句,例如[ condition ] || action $IFS存在的是分隔符号,指定分隔符号之后，循环则根据分隔符来划分字符串]]></content>
  </entry>
  <entry>
    <title><![CDATA[编程通用技巧]]></title>
    <url>%2F2019%2F07%2F25%2F%E7%BC%96%E7%A8%8B%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[记录编程的通用技巧，例如log记录,设计模式等. log文件log文件的作用? 为什么需要log文件，log文件怎么使用，如何查看log文件快读定位到错误.]]></content>
  </entry>
  <entry>
    <title><![CDATA[tmux命令]]></title>
    <url>%2F2019%2F07%2F24%2Ftmux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录一些常用的命名，以便平时查看. Command commands ctrl + b new: 新建session s: 列出所有session \$: 重命名session c: 创建window w: 列出所有windows n: 后一个window p: 前一个window f: 查找window ,: 重命名window &amp;: 关闭当前window %: 垂直分割 “: 水平分割 x: 关闭分割的窗口(&amp;是一样的效果) [: 进入本文模型, 开始复制内容,结束复制 ]: 内容复制到命令行 参考网址Tmux快捷键和速查表]]></content>
  </entry>
  <entry>
    <title><![CDATA[空间碎片项目]]></title>
    <url>%2F2019%2F07%2F23%2F%E7%A9%BA%E9%97%B4%E7%A2%8E%E7%89%87%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[记录空间碎片项目的知识和实验. 正文参考文献getting semi-major from tleOrbital_elementstwo_line_element_setKeplerian-elements-tutorial时间序列预测方法总结IRIDIUM33 and Cosmos 2251 collisionNorad TLE data]]></content>
  </entry>
  <entry>
    <title><![CDATA[git常用命令]]></title>
    <url>%2F2019%2F07%2F23%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录平时使用频率高的git命令. 基本命令 git config: 设置配置文件 git init: 初始化仓库 git clone: 仓库复制 git status: 列举在工作目录中改变文件的状态 git add: 将内容添加到缓存区. git commit -m: 提交内容到版本库中 amend: 合并缓存区的修改和最近一次commit, 生成新的commit替换原有commit(原有commit会删除),如果暂存区没有内容，可修改上一次commit描述. git push/pull: push为提交到远程仓库,pull为从远程仓库获取内容。 git branch -a: 列举所有分支，其中包括远程分支. d xxx: 删除xxx分支 git checkout: 切换到不同分支，’-b’表示新创建分支，并切换到新建分支,在checkout之前需要暂存工作区中的修改，因为checkout到新分区，工作区和暂存区都是发生改变. 对文件进行操作时,更改的工作目录，而不是暂存区. git merge: 合并两个分支 git reset: 回退提交的历史版本. 根据参数来选择是否回退缓存区和工作区 soft: 缓存区和工作区不改变 mixed(默认): 将回退版本的内容同步到暂存区，工作区不受影响.(现将当前分支的暂存区的内容清除，之后再将回退版本的内容不同步到暂存区) hard: 缓存区和暂存区都同步回退版本的内容,换言之, 丢弃所有未提交的内容.(与mixed参数的不同之处在于工作目录会清除和覆盖) 上述参数和HEAD一起使用表示将当前的修改从缓存区(mixed)或者缓存区和工作区一起(hard)删除. HEAD~n: 表示回退n个版本. HEAD^: 上一个版本 对文件进行回退是，soft/mixed/hard参数无效，回退的文件一定会存在暂存区内，不会在工作目录中。 git revert: 不改变提交历史，创建一个新的提交来撤销更改.一般在公共分支上的回滚操作 git log: 查看提交历史 git reflog: 查看所有历史提交，包括被删除的commit记录,reset操作和孤立节点 git rm –cached xxx: 删除暂存区中的xxx文件,不加入cached会将工作目录中的内容也删除. 和git reset HEAD – &lt;xxxfile/xxxdictionary&gt;的区别在于rm将文件从index移除,让文件处于unstaged状态(也就是add之前的状态),如果进行commit，那么在历史版本中也会移除. reset是将HEAD版本的信息同步到index中。 git show: 显示某次提交的详细内容. git diff: 显示未添加到暂存区的内容 –cached: 显示已添加到暂存区,为commit的内容 HEAD^: 和上一个版本的差异 HEAD – ./lib: 和HEAD版本在lib目录山的差异 origin/master..master: 和远程master分支山的差异 git ls-files: 列出暂存区中的文件(一般不用),- git show-branch –all: 图示所有的分支历史 git whatchanged: 显示历史提交对应的文件修改. git mv xx kkk: 将暂存区中xx文件重命名为kkk 常见信息 changes not staged for commit: 说明列举的文件在工作区发生变化，但是没有添加到暂存区. changes to be committed: 文件已经添加到暂存区，没有提交到历史版本中。工作区(working directory)、暂存区(index/stage)和提交历史 Untracked file: 未跟踪的文件，表示还未加入暂存区. 参考网址 git命令大全]]></content>
  </entry>
  <entry>
    <title><![CDATA[时序序列预测分析]]></title>
    <url>%2F2019%2F07%2F22%2F%E6%97%B6%E5%BA%8F%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[近期项目和时序序列预测相关，记录前人的工作。 传统方法 AR模型 MA模型 ARMA模型- 机器学习方法数据预处理 python库: sklearn.preprocessing 采样 样本中每个类别的数据分布不均衡和样本权重不同 无量纲化不同特征的数值大小(纲量)不同，例如10000和10, 采用无量钢化获取同一纲量的特征，从而让数据有可比性. 下面介绍四种Feature scale: Rescaling(min-max normalization):$$x’ = \frac{x-min(x)}{max(x)-min(x)}$$ Mean nomalization:$$x’=\frac{x-mean(x)}{max(x)-min(x)}$$ Standardization (Z-score Normalization)$$x’=\frac{x-mean(x)}{std(x)}$$ Scaling to unit length$$x’=\frac{x}{||x||_2}$$ 标准化和归一化的中文翻译有很大的迷惑性, Standardization是在机器学习算法中广泛被使用的归一化方法,通过每个特征单独计算各自的z-score来达到同一纲量的作用. Scaling to unit length是在机器学习领域计算距离时常用的预处理。 缺失值填充 对样本中的NAN值进行处理 消除NAN值 根据附近的值计算平均值，中位值等 视为特征的feature，但是得确认NAN是有意义的请款下. 特征构造Categorical Features范畴特征是通用的特征,例如文本,字符等. 特征二值化 连续的特征转为为离散的特征. 设定阈值，大于阈值设置为1，小于阈值设置为0. One-hot编码 多类别特征,例如对于某些汉字可表示为名词，动词和形容词，此时用one-hot形式的编码. 在线性算法中使用广泛 设置选取特征值对应的位置为1，其余位置为0. Hash编码避免one-hot编码引起的稀疏性问题. 对特征计算的hash值作为新的特征,再用one-hot编码等. Count 编码统计特征值在训练集中出现的次数. 在线性算法和非线性中使用广泛 结合数据变换(log) 多项式编码考虑不同特征之间的关系(交互).例如XOR问题通过构建交互特征可用线性模型来解决. 扩充编码数据中存在一些基础的特征，需要手工提取分类.例如在浏览器的’user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36’中可提取&#39;Operation System: Windows NT 10.0&#39;,Browser: Chrome等潜在信息. 合并特征多个不同的特征映射到同一个相同特征. 例如机构有简称和全称，可将此转化为相同的特征. Numerical Features Rounding通过四舍五入等方法将数字特征转化为范畴特征，之后用范畴特征的方法来处理. Binning将数字特征分区间，并设置区间id. Scaling将数字特征缩放到指定范围中,有Standard(Z) Scaling, MinMax Scaling, Root Scaling, Log Scaling Interactions编码不同数字特征值之间的相互影响因素,可通过加减乘除来计算特征值之间的影响, 两个特征值之间的差值等. trick 选取线性算法时，对特征进行非线性变换会提升效果, 非线性变换有Polynomial Kernel, Leafcoding(random forest embedding), Genetic algorithms, Locally Linear Embedding, Sepctral Embedding, t-SNE) 统计原始数据中潜在的信息. 例如NAN的数量，0的数量，负数的数量，最大值，最小值，Skewness(偏移度) 时间信息处理: 直接将时间戳作为特征会导致无法获取时间本身存在的循环特征. 例如2019/7/23 23:55和2019/7/24 00:01实际相差只有6分钟,如果直接使用时间戳导致相差为1434分钟，不符合常规. 策略是将时间戳$t$表示为表示为$(x,y)$,其中$x=sin(),y=cos()$. 例如早上六点,表示为$(x=sin(2np.pi6/24),y=cos(2np.pi6/24))$., 形象解释就是将时间戳映射到圆上，从而具有循环的特性。参考文章. 除此之外，根据不同时间区间的目标分布不同，将其转化为Categorical Features. 例如不同季节的消费水平差异大，可将不同季节设置one-hot特征等. 最后也可根据时间特征和目标的关系，对部分时间序列进行编码. 空间特征: 例如GPS位置,国家地名等. 转化为categorical特征: Kriging, K-means clustering, Raw latitude longitude(经纬度), 邮政编码 寻找附近的hubs(中心), 小城镇可能会收到大城市文化的影响。 排除异常行为, 例如同一个可能同时做多趟火车去不同城市。 探索额外特征: 根据目标和数据之间的关系来探索额外的特征. 工具Console, Notebook, Pandas 变换数据: 样本原始分布无规律，对其进行数据变换之后，或许能呈现某种规律。module: sklearn.preprocessing 中的PolynomialFeatures. 常见的变换: log, 指数, Box-Cox NLP相关的特征工程 数据清洗: Lowercasing Unidecode(例如葡萄牙语中的字符转化为英文) 删除非字母数字([a-z][A-Z][0-9]) 修复: 修复编码问题和修剪 Tokenizing: 对标点符号编码: ,.!等标点符号进行编码 Tokenize N-Grams Skip-grams: 跳过一些连续的词进行gram Char-grams: 字符级别的gram Affixes: 前缀词和后缀词 Removing: Stopwords: 删除在停用词表中的词 Rare words: 删除稀有词 Common words: 删除一些通用词，例如’的’ Roots: 矫正拼写 裁剪: 只选取开头的字符 音译(transliteration) 词干(stemming): 例如’cars’转化为’car’ 词形变化(inflection/lemmatisation): 考虑时态和人称,和词干相反。 stemming和lemmatisation区别: stemming是根据字典来查询到对应的词干，不考虑上下文信息,lemmatisation则考虑文本的上下文信息，能够识别同一个词的不同词性. 丰富语义特征: 文档特征: 空格数, tab数，行数，字符数，token数, 文档长度 插入实体: 插入实体将内容补充完整, “我去了爱情海”–&gt;“我去了爱情海(商城)” 句法特征: POS(Part of speech):词性, 考虑词和词之间的关系 文档的阅读水平和类型等: 四级，六级水平 相似度(important): 统计两个词同时出现在文章中的频率 两个文档的距离(Normalized compression distance): 衡量文档的相似度 词之间的距离: Levenshtein/Hamming/Jaccard Distance，编辑距离 词向量之间的距离: word2vec/Glove 词的权重频率 TF-IDF 词出现在文档(title,description)中的频率 降维: PCA, SVD, LDA(TF-IDF followed by SVD), LSA(创建主题向量)LSA代码 额外的模型: 情感分析: 创建表示情感的向量 主题模型: 在主题模型的数据库中学习到主题应用到当前数据中. 案例: 判断两个广告是否重复: 1).广告自身的特征,例如产品名称,价格,LSA,LDANnormalization title length等. 2).文档之间的不同,例如相似度，共现词,文档长度差等 [] 特征选择 python模块: feature_selection [2]从两个方面来选择特征: 特征是否发散: 特征发散说明方差大，特征值分布分散，利于对样本的区分。反之，不利于对样本的区分。 特征和目标的相关性: 和目标相关度高的特征优先被选择。 [2]涉及三种特征选择方法: Filter: 根据特征和目标之间的相关性来选择特征. Wrapper: 根据评价指标(例如F1，AUC等)来选择特征. Embedded: 模型根据其自身学习到的特征权重来选择特征,例如决策树的信息增益. Filter 方差选择法: 计算特征的方差，选择方差大于设定的阈值对应的特征。VarianceThreshold 相关系数法: 计算特征和目标函数的相关系数来选取特征. SelectKBest 卡方检验: 检验定性自变量和定性因变量的相关性. SelectKBest 互信息法: 评价自变量和因变量的相关度。SelectKBest Wrapper 递归特征消除法: 模型通过多次迭代来选择特征，其中每次迭代会消除部分特征. RPE- Embedded 正则化: 通过L1或者L2正则化,让模型自己选择特征. SelectFromModel 决策树: 决策树模型中的信息熵来对特征进行选择, 例如GBDT,XGBoost等模型. 深度学习-- 降维 PCA LDA- 模型和方法 Boosting系列: XGBoost(最有效), LightGBM, GBDT, Adaboost Random Forest: Extra Randomized Trees: SVM: Linear Regression: Logistic Regression: LibLinear in sklearn Neural Networks: 调试模型融合 Voting Averaging Bagging: 有放回去的取出样本来训练多个基础模型，之后将其进行集成(voting/averaging),代表为Random Forest Boosting: 是一种迭代的方式来训练，每次对所有样本进行训练，通过对分类错误的样本更大的权重，从而使模型更加关注于之前分类错误的样本, 代表为AdaBoost. Stacking: 首先k-fold方法将训练集分割成K个数据集${T_{all}/T_1,\cdots ,T_{all}/T_i,\cdots,T_{all}/T_k}$,其中$T_i={X_i,Y_i}$,用K个数据集分别训练K个基础模型，之后层将前一层的输出拼接起来${\hat{Y}_1,\cdots,\hat{Y}_i,\cdots,\hat{Y}_k}$作为当前层模型$F_i$的输入,当前层模型的输出为${Y_1,\cdots,Y_i,\cdots,Y_k}$进行训练，不断迭代。在测试集上的预测值为$\hat{Y}_p=F_i(Mean(\sum_kF_{i-1}(X_p) ))$, 其中$X_p$为测试集的feature,将前一层对$X_p$的预测的平均值作为当前层的输入来预测得到最后的结果. 代码见参考文献[6],为什么效果会好？？？ 参考文献 关于时序序列预测的一些总结 使用sklearn做单机特征工程 feature engineering quora-question-pairs 模型融合方法概述 如何在Kaggle首战中进入前10%]]></content>
  </entry>
  <entry>
    <title><![CDATA[python3常用模块介绍]]></title>
    <url>%2F2019%2F07%2F22%2Fpython3%E6%A8%A1%E5%9D%97%E4%B9%8BOS%E4%B8%8ESYS%2F</url>
    <content type="text"><![CDATA[记录os模块,sys模块,pandas模块的常用方法. python基础知识 list[:,1]和list[:,[1]]区别在于前者将1所在维度删除,而[1]则会保留当前维度. 例如$a \in \mathbb{R}^2$,a[:,1]返回的一维数组，二维数组被返回通过a[:,[1]]. map(fun,iter): 对迭代器(list,tuple etc.)中每个元素调用fun函数，返回结果构成的list. pandas 模块创建对象 Series: index为行索引,可用list初始化 DataFrame: index为行名称,columns为列名称 可用dict初始化,key为列索引, value为具体的值 df.dtypes: 显示每列数据的类型 支持bool type, 例如df.loc[:,df[‘a’]&gt;0], 筛选出’a’行元素大于0的列 行名字又叫做label 显示数据 df.head(number): 前number行数据 df.tail(number): 最后number行数据 df.index: 展示行索引 df.columns: 展示列索引 df.to_numpy(): 将DataFrame转化为numpy类型. 如果DataFrame中包含单一类型, 例如float, 转换速度很快. 如果DataFrame包含多个multiple types, 转换过程比较耗时，并且转化之后数据类型为object, 且不包含index和columns. df.describe(): 展示数据的一些统计变量,例如数量,均值和方差等. df.T: 转置操作 df.sort_values(by=’column1’): 按照列”column1”排序,默认升序. 选择数据 Series类型的[]得到的是常数，DataFrame类型的[]得到的是一列. df.loc[row_name,column_name]: 根据名字(不支持索引)来选择行和列，其中row_name是单独的label或者list类型,单独的label得到的是Series类型,list得到的是DataFrame类型. 例如df.loc[‘A’],df.loc[[‘s’,’b’]],df.loc[[‘a’],[‘d’,’c’]], [符号而非(符号 df.iloc[row_index, column_index]: 根据index索引来选择行和列 df[colname]: []只能用于选择colunms,无法选择行 1&#123;% label danger @不能将df[]内容赋值到df.loc中, df.loc[&apos;A&apos;] = df[&apos;A&apos;]是错误的, 原因??,正确是通过to_numpy()转化. %&#125; df.column_name: 通过属性的方式来获取数据,其中column_name为列名字, 可通过df.column_name的方式修改已经存在列的内容，但是无法添加不存在的列 df[start_index:end_index:step]: 同python语法一样支持slice 通过回调函数(callable)来选择内容, df[callable,:]: 其中callable必须是带一个参数(一般是DataFrame或者Series)的函数，例如: df.loc[lambda df: df.A &gt;0, :] df.at[]和df.iat: 在查找scalar时速度快 df.isin([‘a’,’b’]): 选择符合条件的列, 例如df[df[‘A’].isin([‘a’,’b’])]表示为选择’A’列中值为’a’或者’b’的行. df.get(): 和[]方法最大的区别在于选取无效column_name时，不会报错。 df.values: 和to_numpy()方法作用相同 缺失数据 np.nan 表示确实的数据。 df.dropna(how=’any’): 删除所有缺失的数据. df.fillna(value): 用value填充NAN df.isna(df): 判断DataFrame中的元素是否为NAN,返回boolean类型-- 基础操作 统计操作是将NAN值排除在外. df.mean(axis=0): axis=0 表示行之间, axis=1表示列之间。 df.sub(Series,axis=0): 减法操作 df.apply(lambda x: x): 其中x默认是列(Series) df.value_counts(): 统计不同元素出现的次数. df.str.lower(): 元素转换为小写. df.query(‘a’&gt;’b’): 等价于df[data.a&gt;data.b] df.sort_values([‘a’]): 按照’a’列中的数据来排序. df.agg(func,axis=0): 集成在某个维度上的一个或者多个操作,一般是在groupby之后使用, 例如df.groupby(df[‘a’]).agg([‘max’,’min’])先将数据按照列’a’进行groupby,之后对每个group中求最大值和最小值. df.add(): 两个DataFrame相加 df.drop(): 两个DataFrame相减,(一般用于将一整行或者一整列删除) df.sub(): element-wise相减 合并 pd.concat([data1,data2], axis=0): 其中data1和data2是DataFrame或者Series类型. pd.merge(left,right,on=’name’): 根据name将left和right内容合并 pd.append() DataFrame添加一行或者一列: df.loc[‘xxxx’]= 添加一行, df[‘xxx’] = 添加一列 组合 步骤:1).splitting 2).applying 3).combining df.groupby(‘column_name’).sum(): 按照column_name将’A’列中相同的值组合一起，之后对每个组中的值进行相加操作, 与mysql的groupby类似. df.merge 和 concat的区别在于merge指定相同的key进行合并(只减不增)，而concat是扩展dataframe.(只增不减) pd.melt: 融合, 可以将column names融合为数据. 读取和存储数据 df.csv(‘xxx.csv’): 写入xxx.csv df.read_csv(‘xxx.csv’): 读取xxx.csv文件 index_col: 0表示将第一列设置为index(行索引) df.to_hdf(): 写入HDF5文件 df.read_hdf() df.to_excel() df.read_excel() 方法 df.set_index([a,b,c]): 将某列设置为index(行索引) df.stack(): 从”表格结构”(行列都有索引)变成”树状结构”(行的多重索引) df.unstack(level=): 与stack()相反, 其中level表示需要转化为column的索引，例如0表示第一层的index转化为column df.groupby(id):按照id将dataframe进行group df.agg():在指定的维度上执行一个或者多个操作 Series操作 map: 通过字典将数据进行替换. new_series = series.map(dict) os 模块 This module provides a portable way of using operating system dependent functionality. 常用功能: 文件操作相关 官方文档 os.getcwd(): 获取当前目录 os.walk(): 遍历path下的所有文件和目录, 返回三个元素的tuple (dirpath,dirnames,filenames) os.path.join(path1,[a,b,c]): path1和a,b,c分别联合成完整的路径 os.path.basename(): 只返回文件名,不返回目录路径 os.path.dirname(): 只返回目录路径，不返回文件名 os.path.split(): 返回(dirname,basename) os.path.exits(path): 文件是否存在 os.path.isdir(): 是否是目录 os.path.isfile: 是否是文件 os.mkdir(): 生成单个目录 os.makedirs(‘dirname/dirname3’): 生成多级目录, 等价于”mkdir -p” os.remove(): 删除文件 os.path.getsize(): 文件大小 os.path.isabs: 是否是绝对路径sys 模块 This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. 常用功能: 运行python环境相关 官方网址 sys.argv: 命令行列表参数,其中第一个元素是程序本身的路径 sys.path: python3中module的搜索路径, 引用自定义模块需要append自定义module路径 argparse模块 parser = argparse.ArgumentParser(): 新建参数解析器 parser.add_argument()； 对参数解析器添加参数 xxx: 必填参数，无需指定xxx, 例如python3 argument –xxx: 选填参数, 在输入参数之前需指定–xxx, 例如python3 –xxx agument -xxx: 与–xxx相似 action: 遇到指定参数之后执行的动作,例如add_argument(&#39;--foo&#39;,action=&#39;store_true&#39;)表示遇见--foo参数是设置foo参数的值为True. default: 默认赋给参数的值 type: 指定参数类型 help: 简单描述参数的作用 dest: 如果存在表示指定Namespace(parse_args()的返回对象)中的属性名字，如果不存在dest参数，那么默认xxx作为属性名字. args = parser.parse_args(): 解析解析器中的参数内容到Namespace类 args.xxx: 通过访问Namespace类中属性来获取参数的值 logging模块 当logging模块和tensorflow模块同事出现时，loggging模块无法输出到文件中. tensorflow-logging-do-not-appear-meanly pdg模块12import pdbpdb.set_trace() ＃设置断点 程序会执行到pdb.set_trace(),之后跳出交互界面，输入命令就能对其进行调试, 命令详细信息如下: 命令 含义 b 设置断点 c 程序继续执行 l 查看当前代码片段 s 进入函数 r 执行代码直到返回结果 q 终止并且退出 n 执行下一行 pp 打印变量的值 help 帮助 tbreak 临时断点 cl 清除断点 unt 运行到指定行 j 跳转到指定行,被跳过代码不执行 a 查看函数参数 time模块在此模块中存在一个纪元时间为1970.1.1 00:00:00(UTC) struct_time数据结构: a named tuple. (tm_year,tm_mon,…) mktime(): 将本地的struct_time转化为自从纪元以来的秒数,也就是当前时间点减去纪元时间点，之后转化为秒数 localtime(): 和mktime()方法的作用相反 calendar.timegm(): 将UTC的struct_time转化为从纪元以来的秒数 gmtime():和calendar.timegm()作用相反 strptime(): 将当前时间根据格式专户为struct_time,例如time.strptime(“30 Nov 00”,”%d %d %d”) 占位符12path = &quot;/data&quot;sub_path = &quot;&#123;path&#125;/obj&quot; #/data/obj 字典操作12]]></content>
      <categories>
        <category>python3</category>
      </categories>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究信息抽取的实验室和科研工作者]]></title>
    <url>%2F2019%2F07%2F20%2F%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%E7%89%9B%E4%BA%BA%E5%8F%8A%E5%85%B6%E5%AE%9E%E9%AA%8C%E5%AE%A4-md%2F</url>
    <content type="text"><![CDATA[收集并记录有关信息抽取方向的实验室和科研工作者, 跟随其步伐, 及时发现坑; Main LAB 列举在信息抽取方向取得成果较多的实验室, 信息抽取是实验室一个大方向; 中科院软件所-中文信息处理实验室官方网址中科院软件所信息处理实验室 成员Le Sun: IE.Xianpei Han:He Ben: IR.Hongyu Lin: nested NER, EE.Yaojie Lu: EE.Jialong Tang: Aspect-Level Sentiment Analysis.Cheng Fu: Entity Resolution.Bo Chen: Semantic Parsing.Bo An: knowledege representation. 注: 只列出近期发表文章的一作,加粗表示是指导老师 研究方向实体抽取(NER), 事件抽取(EE), IR等 StatNLP(Singapore University of Technology and Design, SUTD) 重点关注的组, 信息抽取方向很强. 官方网址StatNLP 成员Wei Lu Ruixu Ding: NER.Yanyan Zou: Sequence label.Zhijiang Guo: RE.Zhanming Jie: NER, Semantic Parsing.个人主页Hao Li: sequence label.Bailing Wang: nested NER. 研究方向NER, IR, knowledge representation. Blender Lab (Rensselaer Polytechnic Institute) 中文:伦斯勒理工学院, 跨资源的信息抽取很强，例如文本和图像，文本和语音等联合实体抽取. 官方网址Blender Lab 成员研究方向 从Blender Lab 实验室发的文章而言，low-resource language IE是热门方向cross-source information extraction(IE) NaCTeM(University of Manchester) 中文: 曼切斯特大学 官方网址NaCTeM 成员研究方向NER, IE, IE of Biomedical Science(生物医学的信息抽取) Paul G. Allen School’s NLP (University of Washington) 全称:Paul G. Allen School of Computer Science &amp; Engineering 官方网址:NLP of Allen School 研究方向:IE 代表作:ELMO 中科院自动化所-中文信息抽取实验室官方网站:中文信息处理实验室 研究人员Yubo Chen: even extraction, relation extraction.Kang Liu: even extract, Sentiment AnalysisShaonan Wang: human language representation. 研究方向:EE, RE, HIT-SCIR 哈工大社会计算和信息检索研究中心 研究人员Sendong Zhao: Biomedical Informatics, 个人主页 Xiaocheng Feng: information extraction, text generation. 个人主页 Main People 列举除了上述实验室之外研究信息抽取方向的科研工作者 Westlake University 中文: 西湖大学重点关注, 近期出很多NRE相关的论文,其几乎都在顶会发表，例如ACL, IJCAI等 Yue Zhang: 实体抽取, 中文词分割等. 个人主页, 之前在SUTD工作. 比较有代表性的工作: Lattice NER. Shanghai Key Laboratory of Intelligent Information Processing (Fudan University) 近期大量NER文章出自此实验室, 例如CNN-Based Chinese NER with Lexicon Rethinking Qi Zhang: sequence label 个人主页 TIAL of University of WashingtonTIAL研究IE的大佬:Yi Luan: IE of technical articles, 比较擅长联合多任务，例如联合实体、关系和指代来进行信息抽取. 个人主页, 电气工程系 Nanyang Technological University信息抽取Jing Li: 实体抽取方向，近期他比较关注从entity boundary方向来更好的进行实体抽取. 特别关注他，已经从南洋理工大学博士毕业了。Jing LiJoey Tianyi Zhou: sequence label，一般会开放源代码. 个人主页 大连理工信息抽取实验室Ling Luo: 在医疗领域的实体抽取方法, 个人主页. 比较优秀的期刊:[journal of cheminformatics, Database, Boinformatics], 医疗领域优秀的workshop:[BioCreative, CCKS] The Ohio State University 俄亥俄州立大学Thai-Hoang Pham: Fine-Grained Named Entity Recognition, 对于越南语开展了一系列sequence label的工作。个人主页 Cornell University 康奈尔大学 Arzoo Katiyar: Named entity recognization, Extraction of Entity Mentions等,个人主页 University of Illinois at Chicago 伊利诺大学芝加哥分校实体抽取,槽填充和意图识别任务, 用Capsule Neural Netwoks来解决 意图识别和槽填充任务是任务型对话的重要的一个环节，其中意图识别是识别用户的意图，槽填充则是通过标记机制(例如:BIO scheme)识别用户表达的具体信息，例如时间，地点等. 举个例子,用户阐述”我今天要打车去天安门”，其中意图识别是用户打车的意图，槽填充是通过对语句打标签来识别内容,例如”B-PER B-Time I-Time …”。Congying Xia: 实体抽取, 论文列表, github, 代表作: Multi-Grained-NERChenwei Zhang: sequence label, 主要在实体抽取和槽填充任务. 个人主页, 代表作: Joint Slot Filling and Intent Detection via Capsule Neural Networks. TianJing UniversityMeishan Zhang: opinion Mining, Tagging等. 个人主页 RiTUAL (Research in Text Understanding and Analysis of Language) University of Houston (休斯顿大学)Gustavo Aguilar: named entity recognition in noisy data 个人主页 The ALTA InstituteMarek Rei: 个人主页, sequence label会发一些文章. University of Trento 中文: 特伦托大学 Lingzhen Chen: 实体抽取方向. 个人主页 Institute of Electronics, Chinese Academy of SciencesGuangluan Xu: 信息抽取方向. 论文列表 Institute of Information Engineering, Chinese Academy of Sciences, Beijing 在IEEE发表文章较多。Longtao Huang: information extraction, 涉及一些非监督的方法. 论文列表 Google ResearchGenady Beryozkin: 实体识别, 主要关注是否有一些新方向. 论文列表 MSRAFangzhao Wu: 中文实体识别, 中文分词任务. 个人主页 Fudan UniversityXipeng Qiu: 大老板 个人主页Xuanjing Huan: 大老板论文列表 Xinchi Chen: 中文分割, 文本生成, 自然语言推导. 个人主页 自然语言推导: 给定前提$sentence_q$和假设$sentence_2$，判断上述两个句子的关系属于矛盾(contradiction)、中立(neutral)和蕴涵(entailment)三者中的哪一个。 Jingjing Gong: 擅长用attention机制,例如Capsule network, 来研究中分分词, 文本分类等.Penfei Liu: sequence label. 个人主页 CS, Cornell UniversityArzoo Katiyar: 实体抽取和关系抽取. 个人主页 Shanghai Jiaotong UniversityAPEX Data and Knowledge Management LabShaodian Zhang: 医疗领域的NLP(包括实体抽取和embbeding等) University of Arizona 中文: 亚利桑那大学Vikas Yadav: 实体抽取方向.论文列表 Tsinghua UniversityHui Chen: 实体抽取和图像，有点杂. 论文列表 CS of Carnegie Mellon UniversityZhilin Yang: Sequence label是其一个方向，Zhilin Yang AlibabaShan Yan: 在序列标注方向比较厉害。论文列表 WorkshopWorkshop on Noisy User-generated Text:官方网址, WNUT是由ACL协会举办, 信息抽取(特别是实体识别)是其重要的一个关注的方向。]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch技巧]]></title>
    <url>%2F2019%2F07%2F17%2Fpytorch%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[技巧 只计算部分tokens，将无关tokens mask设置为-9999999 将索引矩阵转化为one-hot矩阵, torch.eye + torch.index_select: 从对角矩阵中选择index对应的one-hot形式。 hinge loss funciton: 选择ground-truth对应的token的值, torch.gather 选择次大的token对应的值,先将ground-truth转化为one-hot, 之后将one-hot转化为-9999999,最后获取次大值 将上述两者的值相减g 再加入margin的值]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim技巧.md]]></title>
    <url>%2F2019%2F07%2F17%2Fvim%E6%8A%80%E5%B7%A7-md%2F</url>
    <content type="text"><![CDATA[窗口分割1234567891011121314水平分割当前文件crt + w s竖直分割当前文件crt + w v水平分割另一文件filename:sp filename竖直分割另一文件filename:vsp filename分割窗口之间切换:crt + w [h,j,k,l] tab标签12345678910新建标签:tabedit filename查看当前标签情况：tabs标签之间切换:&apos;index&apos; + gt (index 表示索引)gt 向后切换一个Gt 向前切换一个 统计字符串:%s/xxxx//gn:统计xxxx在文本中出现的次数]]></content>
  </entry>
  <entry>
    <title><![CDATA[博客的配置和写作教程]]></title>
    <url>%2F2019%2F07%2F12%2Fhexo%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7-md%2F</url>
    <content type="text"><![CDATA[由于写博客过程需基本的markdown知识和配置next主题，本博客简单介绍常用的一些markdown技巧和next主题的配置; markdown技巧分隔线12分隔线&lt;hr /&gt; 分隔线 12***分割线 分割线 引用123&lt;blockquote&gt; 引用内容1 &lt;blockquote&gt; &gt; 引用内容2 &gt;&gt; 引用内容3 引用内容1 引用内容2 引用内容3 注释1&lt;!-- 注释 --&gt; 字体大小和颜色1&lt;font color=&quot;&quot; size=&quot;&quot;&gt; 内容 &lt;/font&gt; Font Awesome通过设置$class$选择图标，使用方法和$boostrap$类似 代码块及其高亮行间代码12```+language```&lt;!-- end --&gt; 自动识别语言并高亮代码 行内代码1`inline code` inline code 换行段落和段落之间换行通过空行实现，而两个空格+回车对段落中间强制换行 列表123456789无序列表+ item1+ item2- item3- itme4* item5* item6 item1 item2 item3 itme4 item5 item6 超链接123456行内式:[paulpig的博客](https://wbc520wrz.cn/)参考式:[paulpig的博客][1][1]:https://wbc520wrz.cn 行内式:paulpig的博客 参考式:paulpig的博客 注:空行不可少 图片12paulpig的图片![paulpig&apos;s picture](/assets/blogImg/paulpig.png) paulpig的图片 注:根目录为”source/“ 转义字符\为转义字符 表格123| 左对齐 | 右对齐 | 居中 || :--- | ---: | :---: || content | content | content | 左对齐 右对齐 居中 content content content 其中:---,---:和:---:分别表示左对齐、右对齐和居中 12345678910111213141516&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; 左对齐 &lt;\th&gt; &lt;th&gt; 右对齐 &lt;\th&gt; &lt;th&gt; 居中 &lt;\th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt; content &lt;\th&gt; &lt;th&gt; content &lt;\th&gt; &lt;th&gt; content &lt;\th&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 左对齐 右对齐 居中 content content content 注:引入&lt;table&gt;标签时,需删除中间的空格和回车,例如&lt;table&gt;&lt;thead&gt;&lt;tr&gt;,原因不清楚,图方便可在excel中填写内容，之后转化为xml hexo/next 配置label标签1&#123;% label dander @content %&#125; content 其中danger可选项有default | primary | success | info | warning | danger注:对文本中不同内容显示不同状态 buttion标签1&#123;% btn #, text , home fa-4x %&#125; text 其中text,home fa-4x分别表示buttion内的文本和图标.注:阐述不同方法时使用到tag标签 tag标签1234567891011&#123;% tabs Fourth unique name,2 %&#125;&lt;!-- tab Solution 1 @text-width --&gt;**This is Tab 1.**&lt;!-- endtab --&gt;&lt;!-- tab Solution 2 @text-width --&gt;**This is Tab 2.**&lt;!-- endtab --&gt;&lt;!-- tab Solution 3 @text-width --&gt;**This is Tab 3.**&lt;!-- endtab --&gt;&#123;% endtabs %&#125; Solution 2]]></content>
      <categories>
        <category>writing</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不均匀光照的二值化方法（去阴影、去曝光）]]></title>
    <url>%2F2017%2F03%2F03%2F%E4%B8%8D%E5%9D%87%E5%8C%80%E5%85%89%E7%85%A7%E7%9A%84%E4%BA%8C%E5%80%BC%E5%8C%96%2F</url>
    <content type="text"><![CDATA[自动识别”湖南大学迎新系统”的验证码，掌握识别的基本步骤； 测试图像是白底黑字一、算法思想1.计算整幅图像的背景图像2.图像的标准化3.对比原图像和背景图像，增强和对比度。二、算法详细步骤1.计算整幅图像的背景图像(1)设计w*w的区域（w自定义大小）(2)将(1)的区域去遍历整幅图像，由于是白底黑字的图片，将区域中最高的五个像素点的平均值作为背景值。原图背景图像2.图像的标准化简单来说就是将背景点设置为255，将有效点设置为某个区间的值。（目的是为了之后二值化的时候，更好的确定阈值）3.对比原图像和背景图像，增强和对比度基本思想就是局部区域中背景和原图的对比度小，将对比度增强。关键是对公式的理解：详解：(1)参数说明：Ib(x,y)代表背景图像在(x,y)点的像素值Is(x,y)代表原图像在(x,y)点的像素点Ie(x,y)代表经过去除背景之后的图像Bj的值是255，代表将背景像素点转化为255k是一个分段函数，代表不同区域的背景和原图的对比度的缩放（也就是对比度增强的倍数）(2)公式解释1.1否则,Ie(x,y)=Bj;也就是原图的像素点大于背景图像的像素点，由于图像是白底黑字，此像素点一定是背景像素点，直接设置为255.2.1Ie(x,y)=Bj-k(Ib(x,y)-Is(x,y));如果Ib(x,y)&gt;Is(x,y),此原图的像素点是有效像素点，根据像素点大小判断对比的缩放比例。例如在阴影很大（或者曝光很大）的地方，对比度（k）要增大。3.12若（Ie(x,y)&lt;0.75Bj）set Ie(x,y)=0.75Bj;将原图中的有效像素点设置为[0.75Bj,Bj)中的值，为了之后二值化的时候更好的设置阈值.三、算法具体实现大牛的博客最后效果图四、参考文献不均匀光照文本图像的二值化 —贺志明五、代码下载]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>preprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抗打印的水印算法]]></title>
    <url>%2F2017%2F01%2F18%2F%E6%8A%97%E6%89%93%E5%8D%B0%E7%9A%84%E6%B0%B4%E5%8D%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[将水印信息(周期函数旋转一定角度)嵌入到分块图像中; 一、算法的思想主要思想：周期图像的自相关函数，水印信息实际上是角度 1.嵌入水印思想：将一个有周期性并且有角度的随机块嵌入到图像中。举个例子：现有28*7的随机矩阵，之后将此矩阵不断重复知道能够覆盖图像，之后按照一个角度（实际上就是我们的水印信息）将此随机矩阵旋转，最后嵌入到原图像中。这时候，可能有人会问，这么简单粗暴的将水印嵌入图像中，不是将原来图像的信息给覆盖了吗？不要急，我们之后会提到如何解决此问题（主要引入一个JND概念）。 2.提取水印的思想：将加水印图像的噪声提取出来，之后将“噪声”做自相关，最后通过hough直线检测就能将我们的角度提取出来。注意点：一开始读论文的时候，我一直不能理解为什么将嵌入水印的图像自相关之后就能得到结果，此时加水印的图像不是周期图像。后来我才明白其实可以简单理解为加入的水印其实就是噪声，我们其实提取的是加水印图像的噪声。 二、算法的详细步骤1.嵌入水印（随机块旋转指定角度，嵌入图像）![嵌入算法总览图(http://obkykcznj.bkt.clouddn.com/in_watermasking.png)1) RGB图像转化为YCbCr图像，之后提取Y分量，之后都是对Y分量操作的。2) 预处理，为了破坏原图像存在的周期性。3) 提取图像的JND，JND可以简单理解为像素点能让人眼识别不出最大的阈值。4) 将图像分成九块，八块嵌入水印信息，一块是参考块。5) 随机产生26个随机{0,1}集合,之后进行汉明码编码产生32位水印信息。并且以四位计算出一个角度，一共有八个角度。6) 随机产生28*7随机块，不断重复随机块，直到能覆盖图像块，之后按照步骤(5)产生的角度进行旋转。7) 旋转之后的随机块，同时结合JND，嵌入到原图像块中。 2.提取水印：1) 原图像减去维纳滤波之后的图像，得到的就是我们嵌入的信息。2) 将图像分成九块，分块操作。3) 对图像每块进行自相关，然后进行高斯变换去噪。4) 对步骤(3)得到的矩阵，提取峰值。5) 通过hough直线检测，检测到直线的角度6) 将步骤(5)得到的角度，转换为二进制信息，之后再通过汉明码检错得到水印信息。 三、算法的实现嵌入水印过程原图像： 图像的JND： 嵌入的随机矩阵： 嵌入水印图像： 打印之后，相机拍摄的图像： 提取水印过程自相关之后的矩阵，从图中看出极值点是在几条平行的直线上 提取极值点之后的图像，从图中能够看到直线 提取水印效果：提取出的32位中出现了两位的错误，也就是检测九块中出错了一块。 四、参考文献Toward an interactive poster using digital watermarking and a mobile phone camera 五、代码下载]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python+svmlib+opencv实现图片验证码的自动识别]]></title>
    <url>%2F2016%2F11%2F07%2Fpython-svmlib-opencv%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E9%AA%8C%E8%AF%81%E7%A0%81%E7%9A%84%E8%87%AA%E5%8A%A8%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[自动识别湖南大学迎新系统的验证码，掌握识别的基本步骤； python配置opencv网址python配置SVMLIB网址 一、思想（基本步骤）1.获取训练样本（python“爬”取）2.图片预处理（去噪声）3.图片分割（分割为单个数字）4.提取特征（每行黑色像素点构成的向量的协方差）5.训练（基于svmlib）6.识别（基于svmlib）二、获取训练样本 图片验证码是来源于“湖南大学迎新系统” 1234url='http://yxxt.hnu.cn/VerifyCode.aspx?????' f = open("./picture/"+str(i)+".png","wb") f.write(urllib2.urlopen(url).read()) f.close() 代码分析：上述url是网站中获取验证码路径，并且将获取的图片保存到本地 三、图片预处理（去噪声）通过观察训练样本，发现噪声点的RGB都大于等于195 1234for i in range(image_output.shape[0]): for j in range(image_output.shape[1]): if(image_output[i][j][0]&gt;=195 and image_output[i][j][1]&gt;=195 and image_output[i][j][2]&gt;=195): image_output[i][j]=(255,255,255) 代码分析：由于从上一步获取的图片都是白色作为背景色，因此将噪声点全部设置为白色 四、图片分割（分割为单个数字）基本思路：首先得分析图像，此样本中的图片的每个数字基本上分布比较均匀，只要确定数字的左右、上下边界，就能将单个数字分割出来 1234567child_img_list = []for i in range(4): x1 = 10 + i * 50 x2 = 60 + i * 50 child_img = img[15:100,x1:x2] child_img_list.append(child_img) cv2.imwrite(path+str(PictureNum)+str(i)+".png",child_img) 代码分析：通过分析图片，发现图片中每个数字的高度固定在15~100（上边界和下边界）之间，而x1代表数字的左边界，x2代表数字的右边界 五、提取特征（每行黑色像素点构成的向量的协方差）1.将图片分割成5×5个子区域，每个区域是10×17个像素点2.计算每个子区域中黑色像素点个数，得到了5×5的矩阵3.计算第二步得到的矩阵的协方差（协方差代表了每个子区域之间的关联性）123456789101112FeatureKeep =[[] for i in xrange(5)]for i in range(5): for j in range(5): black_sum=0 for k in range(10): for m in range(17): # 判定黑点 if(input_image[5*i+k,j*5+m] &lt; 200): black_sum=black_sum+1 FeatureKeep[i].append(black_sum)#计算协方差cov_mat = np.cov(FeatureKeep,rowvar=0) 代码分析：FeatureKeep保存了每个子区域的总的黑色像素点个数，cov_mat保存了协方差 六、训练（基于svmlib）123y, x = svm_read_problem("./data/train.txt")model = svm_train(y, x)svm_save_model("./model", model) 代码分析：(详情见参考网址中的svmlib for python详解)1.“train.tx”有特定格式：label index1:value1 index2:value2 …2.y代表样本的标签（数字0类、数字1类），x代表了样本的特征3.svm_train训练得到一个模型4.svm_save_model保存训练模型 七、识别（基于svmlib）123yt, xt = svm_read_problem(“./testPicture/recognize.txt”)model = svm_load_model("./model")p_label, p_acc, p_val = svm_predict(yt, xt, model) # p_label即为识别的结果 代码分析：(详情见参考网址中的svmlib for python详解)1.recognize.txt中是和训练中的train.txt相同格式的2.yt代表了识别图片的编号，就是代表第几张图片。xt代表识别图片的特征3.svm_load_model加载训练模型4.svm_predict识别，p_label中存放的是识别的结果，会返回识别的标签（数字0或者数字1…） 八、参考网址python实现验证码识别svmlib for python详解]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>svmlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于贝叶斯变换的手写识别数字（MFC+Opencv）]]></title>
    <url>%2F2016%2F11%2F02%2F%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8F%98%E6%8D%A2%E7%9A%84%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[基于贝叶斯模型识别书写数字,掌握贝叶斯模型的基本流程; 一、什么是贝叶斯公式P(Wi/X)=P(X/Wi)*P(Wi)/P(X) 二、贝叶斯公式和手写识别的关系（解释上述公式） Wi:分类器（相当于数字’0’类、数字’1’类等） X：输入样本（相当于输入识别的图片，样本是由特征组成在，也就是样本和特征有时候是可以等价理解的） P(Wi/X):当前输入样本X，将样本X归到Wi类的概率值。（越大，说明越有可能归到此类中） P(X/Wi):在Wi类下面，样本值（特征）为X的概率。（已知一个人，此人有头发（特征）的概率） P(Wi):Wi类下的样本数目占总样本数目的百分比(训练样本中，数字’0’类的样本数目有100张，总的样本数目为1000张，那么P(Wi)=100/1000) P(X):可以展开为P(X/W1)*P(W1)+P(X/W2)*P(W2)+······+P(X/Wi)*P(Wi),某个样本（特征）的概率总和 三、具体实现步骤1.预处理采用的是只是添加了一个高斯过滤器（GaussianBlur） 2.图像分割图像分割成7*7个子区域，每个子区域就是一个特征。 3.特征提取图像在上一步中已经分割成77个子区域，每块区域的黑色像素点多于总的黑色像素点在每块区域的均值（总的像素点/（77））就设置为1，反之设置为0；（此处的阈值可以自定义，不一定是要均值，但是可以参考均值） 4.图像训练训练关键的两个部分： 1.P(X/wi)的计算：由于X代表的特征向量，我们暂且将每个特征都假定为独立变量，也就是P(X/wi)=P(x1/wi)* P(x2/wi)* P(x3/wi)*…………，现在我们的问题在于如何求解P(x1/wi)、P(x2/wi)等。在之前对训练样本做了特征提取之后，假定我们的样本数目为N，我们得到了N*25的矩阵，一行代表每张图像的特征向量，每列代表同一个特征，而我们要求的P(x1/wi)就是在同一列中指定的几行中值为1的数量除以指定的几行的行数。其中同一列代表了同一特征（x1），而制定的几行代表了是同一个分类（wi）。2.P(wi)的计算：这个相当简单的，就是每类的数量除以总样本的数量。5.图形识别分别计算P(wi/X)概率，哪类概率大就是判定为分到哪类中。 此处的关键点：首先将图像进行特征提取，当第j个特征值为1的时候的概率为P(xj/wi)，而当特征值为0的时候的概率为1-P(xj/wi).(P(xj/Wi)已经在训练中计算出来) 四、界面展示识别界面 识别成功界面 五、源代码下载手写识别数字源代码下载 数字样本数据下载]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>MFC</tag>
      </tags>
  </entry>
</search>
